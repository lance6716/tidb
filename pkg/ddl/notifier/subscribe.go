// Copyright 2024 PingCAP, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package notifier

import (
	"context"
	goerr "errors"
	"fmt"
	"time"

	"github.com/pingcap/errors"
	sess "github.com/pingcap/tidb/pkg/ddl/session"
	"github.com/pingcap/tidb/pkg/kv"
	"github.com/pingcap/tidb/pkg/lightning/log"
	"github.com/pingcap/tidb/pkg/sessionctx"
	"go.uber.org/zap"
)

// SchemaChangeHandler function is used by subscribers to handle the
// SchemaChangeEvent generated by the publisher (DDL module currently). It will
// be called at least once for every SchemaChange. The sctx has already started a
// pessimistic transaction and handler should execute exactly once SQL
// modification logic with it. After the function is returned, subscribing
// framework will commit the whole transaction with internal flag modification to
// provide exactly-once delivery. The handler will be called periodically, with
// no guarantee about the latency between the execution time and
// SchemaChangeEvent happening time.
//
// The handler function must be registered by RegisterHandler before the
// ddlNotifier is started. If the handler can't immediately serve the handling
// after registering, it can return nil to tell the ddlNotifier to act like the
// change has been handled, or return ErrNotReadyRetryLater to hold the change
// and re-handle later.
type SchemaChangeHandler func(
	ctx context.Context,
	sctx sessionctx.Context,
	change *SchemaChangeEvent,
) error

// ErrNotReadyRetryLater should be returned by a registered handler that is not
// ready to process the events.
var ErrNotReadyRetryLater = errors.New("not ready, retry later")

// HandlerID is the type of the persistent ID used to register a handler. Every
// ID occupies a bit in a BIGINT column, so at most we can only have 64 IDs. To
// avoid duplicate IDs, all IDs should be defined in below declaration.
type HandlerID int

const (
	TestHandlerID HandlerID = iota
)

// RegisterHandler must be called with an exclusive and fixed HandlerID for each
// handler to register the handler. Illegal ID will panic. RegisterHandler should
// not be called after the global ddlNotifier is started.
//
// RegisterHandler is not concurrency-safe.
func RegisterHandler(id HandlerID, handler SchemaChangeHandler) {
	intID := int(id)
	if intID < 0 || intID >= 64 {
		panic(fmt.Sprintf("illegal HandlerID: %d", id))
	}

	globalDDLNotifier.handlers[id] = handler
}

type ddlNotifier struct {
	ownedSCtx    sessionctx.Context
	store        Store
	handlers     map[HandlerID]SchemaChangeHandler
	pollInterval time.Duration
}

var globalDDLNotifier *ddlNotifier

// InitDDLNotifier initializes the global ddlNotifier. It should be called only
// once and before any RegisterHandler call. The ownership of the sctx is passed
// to the ddlNotifier.
func InitDDLNotifier(
	sctx sessionctx.Context,
	store Store,
	pollInterval time.Duration,
) {
	globalDDLNotifier = &ddlNotifier{
		ownedSCtx:    sctx,
		store:        store,
		handlers:     make(map[HandlerID]SchemaChangeHandler),
		pollInterval: pollInterval,
	}
}

// Start starts the ddlNotifier. It will block until the context is canceled.
func (n *ddlNotifier) Start(ctx context.Context) error {
	ctx = kv.WithInternalSourceType(ctx, kv.InternalDDLNotifier)
	ticker := time.NewTicker(n.pollInterval)
	defer ticker.Stop()
	for {
		select {
		case <-ctx.Done():
			return nil
		case <-ticker.C:
			if err := n.processEvents(ctx); err != nil {
				log.FromContext(ctx).Error("Error processing events", zap.Error(err))
			}
		}
	}
}

func (n *ddlNotifier) processEvents(ctx context.Context) error {
	events, err := n.store.List(ctx, sess.NewSession(n.ownedSCtx), 100)
	if err != nil {
		return errors.Trace(err)
	}

	for _, event := range events {
		for handlerID, handler := range n.handlers {
			if err2 := n.processEventForHandler(ctx, event, handlerID, handler); err2 != nil {
				if !goerr.Is(err2, ErrNotReadyRetryLater) {
					log.FromContext(ctx).Error("Error processing event",
						zap.Int64("ddlJobID", event.ddlJobID),
						zap.Int64("multiSchemaChangeSeq", event.multiSchemaChangeSeq),
						zap.Int("handlerID", int(handlerID)),
						zap.Error(err2))
				}
				continue
			}
		}
		// TODO(lance6716): remove the processed event after all handlers processed it.
	}

	return nil
}

func (n *ddlNotifier) processEventForHandler(
	ctx context.Context,
	change *schemaChange,
	handlerID HandlerID,
	handler SchemaChangeHandler,
) (err error) {
	if (change.processedByFlag & (1 << handlerID)) != 0 {
		return nil
	}

	se := sess.NewSession(n.ownedSCtx)

	if err = se.Begin(ctx); err != nil {
		return errors.Trace(err)
	}
	defer func() {
		if err == nil {
			err = errors.Trace(se.Commit(ctx))
		} else {
			se.Rollback()
		}
	}()

	// TODO: Should we attach a timeout to this ctx?
	if err = handler(ctx, n.ownedSCtx, change.event); err != nil {
		return errors.Trace(err)
	}

	newFlag := change.processedByFlag | (1 << handlerID)
	if err = n.store.UpdateProcessed(
		ctx,
		se,
		change.ddlJobID,
		change.multiSchemaChangeSeq,
		newFlag,
	); err != nil {
		return errors.Trace(err)
	}
	change.processedByFlag = newFlag

	return nil
}
